taskname: '+ MobileViTv2-2.0 IN21K'
common:
  run_label: "train"
  log_freq: 500
  auto_resume: true
  mixed_precision: true
  tensorboard_logging: false
  grad_clip: 10.0
  channels_last: true
dataset:
  root_train: "/mnt/imagenet21k_train"
  root_val: "/mnt/imagenet21k_val"
  name: "imagenet"
  category: "classification"
  train_batch_size0: 128 # Effective batch size is 4k (128 * 8 GPUs/node * 8 nodes)
  val_batch_size0: 100
  eval_batch_size0: 100
  workers: 8
  persistent_workers: true
  pin_memory: true
image_augmentation:
  random_resized_crop:
    enable: true
    interpolation: "bicubic"
  random_horizontal_flip:
    enable: true
  rand_augment:
    enable: true
  random_erase:
    enable: true
    p: 0.25
  mixup:
    enable: true
    alpha: 0.2
  cutmix:
    enable: true
    alpha: 1.0
  resize:
    enable: true
    size: 288 # shorter size is 288
    interpolation: "bicubic"
  center_crop:
    enable: true
    size: 256
sampler:
  name: "batch_sampler"
  bs:
    crop_size_width: 256
    crop_size_height: 256
loss:
  category: "classification"
  classification:
    name: "cross_entropy"
    cross_entropy:
      label_smoothing: 0.1
optim:
  name: "adamw"
  weight_decay: 0.05
  no_decay_bn_filter_bias: true
  adamw:
    beta1: 0.9
    beta2: 0.999
scheduler:
  name: "cosine"
  is_iteration_based: false
  max_epochs: 80
  cosine:
    max_lr: 3.e-4
    min_lr: 1.e-6
model:
  classification:
    name: "mobilevit_v2"
    mitv2:
      width_multiplier: 2.0
      attn_norm_layer: "layer_norm_2d"
    activation:
      name: "swish"
    finetune_pretrained_model: true
    n_pretrained_classes: 1000
  normalization:
    name: "batch_norm"
    momentum: 0.1
  activation:
    name: "swish"
  layer:
    global_pool: "mean"
    conv_init:  "kaiming_normal"
    conv_init_std_dev: 0.02
    linear_init: "trunc_normal"
    linear_init_std_dev: 0.02
ema:
  enable: true
  momentum: 0.0005
stats:
  val: [ "loss", "top1", "top5" ]
  train: ["loss"] # we can't use top-1 because some of the transforms (e.g., Mixup) change the labels
  checkpoint_metric: "top1"
  checkpoint_metric_max: true
